{
  "model_info": "DistributedDataParallel(\n  (module): YearProcessor(\n    (inner): CustomTransformer(\n      (pre_conv): Sequential(\n        (0): Conv1d(70, 1024, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(1024, 384, kernel_size=(1,), stride=(1,))\n        (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (pos_enc): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): TransformerEncoder(\n        (layers): ModuleList(\n          (0-3): 4 x TransformerEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n            )\n            (linear1): Linear(in_features=128, out_features=512, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear2): Linear(in_features=512, out_features=128, bias=True)\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (post_conv): Sequential(\n        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(256, 15, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n)",
  "train_loss": [
    4.55665397644043,
    4.064043641090393,
    3.8956929445266724,
    3.8121542930603027,
    3.732281446456909,
    3.6384516954421997,
    3.53527295589447,
    3.4286526441574097,
    3.335042953491211,
    3.2566077709198,
    3.1901997327804565,
    3.128892660140991,
    3.072434425354004,
    3.0162185430526733,
    2.9630738496780396,
    2.9129921197891235,
    2.867602229118347,
    2.8255791664123535,
    2.788257122039795,
    2.75421941280365,
    2.7227344512939453,
    2.6908408403396606,
    2.660389542579651,
    2.6313509941101074,
    2.6039535999298096,
    2.5792319774627686,
    2.556884527206421,
    2.533700942993164,
    2.512341022491455,
    2.4927937984466553
  ],
  "val_loss": [
    2.1154794796388976,
    1.8595237163864837,
    1.7930537263615702,
    1.766715509452078,
    1.728306031531217,
    1.6726137332618236,
    1.6074470285022138,
    1.5415872349574857,
    1.4780491302588157,
    1.4127526194617457,
    1.3463292287320507,
    1.2935988118529929,
    1.2617301983644769,
    1.2382987329166155,
    1.215309333626409,
    1.1874670165305843,
    1.1609694905959222,
    1.1412951442508066,
    1.1199530292834554,
    1.100408765164261,
    1.083593665192626,
    1.0653494640462557,
    1.052790249085852,
    1.0420497253157046,
    1.0310977752301462,
    1.0222340429239736,
    1.0152011582323788,
    1.0076863947647567,
    0.9983147414002035,
    0.9931678556076878
  ],
  "val_loss_batches": [],
  "val_loss_steps": [],
  "batch_loss": [
    4.55665397644043,
    4.064043641090393,
    3.8956929445266724,
    3.8121542930603027,
    3.732281446456909,
    3.6384516954421997,
    3.53527295589447,
    3.4286526441574097,
    3.335042953491211,
    3.2566077709198,
    3.1901997327804565,
    3.128892660140991,
    3.072434425354004,
    3.0162185430526733,
    2.9630738496780396,
    2.9129921197891235,
    2.867602229118347,
    2.8255791664123535,
    2.788257122039795,
    2.75421941280365,
    2.7227344512939453,
    2.6908408403396606,
    2.660389542579651,
    2.6313509941101074,
    2.6039535999298096,
    2.5792319774627686,
    2.556884527206421,
    2.533700942993164,
    2.512341022491455,
    2.4927937984466553
  ],
  "batch_step": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29
  ],
  "epoch_edges": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30
  ],
  "samples_seen": 705600,
  "lr_values": [
    8.999999421943739e-05,
    8.999997687775117e-05,
    8.99999479749463e-05,
    8.999990751103103e-05,
    8.999985548601692e-05,
    8.999979189991883e-05,
    8.999971675275488e-05,
    8.999963004454654e-05,
    8.999953177531857e-05,
    8.999942194509901e-05,
    8.999930055391921e-05,
    8.999916760181383e-05,
    8.999902308882082e-05,
    8.999886701498143e-05,
    8.999869938034021e-05,
    8.999852018494503e-05,
    8.999832942884702e-05,
    8.999812711210066e-05,
    8.999791323476368e-05,
    8.999768779689713e-05,
    8.999745079856537e-05,
    8.999720223983607e-05,
    8.999694212078018e-05,
    8.999667044147194e-05,
    8.99963872019889e-05,
    8.999609240241192e-05,
    8.999578604282517e-05,
    8.999546812331608e-05,
    8.99951386439754e-05,
    8.999479760489721e-05,
    8.999444500617886e-05,
    8.999408084792097e-05,
    8.999370513022753e-05,
    8.999331785320576e-05,
    8.999291901696623e-05,
    8.999250862162281e-05,
    8.999208666729261e-05,
    8.99916531540961e-05,
    8.999120808215704e-05,
    8.999075145160248e-05,
    8.999028326256275e-05,
    8.998980351517152e-05,
    8.998931220956573e-05,
    8.998880934588563e-05,
    8.998829492427476e-05,
    8.998776894487998e-05,
    8.998723140785142e-05,
    8.998668231334253e-05,
    8.998612166151006e-05,
    8.998554945251407e-05,
    8.998496568651788e-05,
    8.998437036368812e-05,
    8.998376348419477e-05,
    8.998314504821103e-05,
    8.998251505591345e-05,
    8.998187350748189e-05,
    8.998122040309946e-05,
    8.998055574295261e-05,
    8.997987952723106e-05,
    8.997919175612786e-05
  ],
  "lr_steps": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49,
    50,
    51,
    52,
    53,
    54,
    55,
    56,
    57,
    58,
    59
  ],
  "mb_train": {},
  "mb_val": {}
}