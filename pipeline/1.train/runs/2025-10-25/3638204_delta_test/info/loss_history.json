{
  "model_info": "DistributedDataParallel(\n  (module): YearProcessor(\n    (inner): CustomTransformer(\n      (pre_conv): Sequential(\n        (0): Conv1d(70, 1024, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(1024, 384, kernel_size=(1,), stride=(1,))\n        (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (pos_enc): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): TransformerEncoder(\n        (layers): ModuleList(\n          (0-3): 4 x TransformerEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n            )\n            (linear1): Linear(in_features=128, out_features=512, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear2): Linear(in_features=512, out_features=128, bias=True)\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (post_conv): Sequential(\n        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(256, 15, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n)",
  "train_loss": [
    20.497126579284668,
    18.406490325927734,
    16.79607582092285,
    15.482303619384766,
    14.361716270446777,
    13.393843650817871,
    12.551642894744873,
    11.807844638824463,
    11.165555953979492,
    10.618160724639893,
    10.152761459350586,
    9.757619380950928,
    9.405941009521484,
    9.09318733215332,
    8.811490535736084,
    8.561145782470703,
    8.332212924957275,
    8.121989250183105,
    7.919856548309326,
    7.735702276229858,
    7.561567783355713,
    7.3955302238464355,
    7.2381932735443115,
    7.084268093109131,
    6.946149826049805,
    6.805155515670776,
    6.677734136581421,
    6.553185701370239,
    6.436058044433594,
    6.332939386367798
  ],
  "val_loss": [
    18.02439612369148,
    16.922047924022284,
    15.736681438952075,
    14.404216112409319,
    13.018994593133732,
    11.713909986797644,
    10.579188413036112,
    9.63840398946587,
    8.876452414478575,
    8.273505183385343,
    7.793859273621014,
    7.397559519811552,
    7.063417000065044,
    6.774993079110068,
    6.530104693466304,
    6.322122890973578,
    6.137994094709961,
    5.973703963537606,
    5.814264624641866,
    5.664484069237903,
    5.515742687850582,
    5.374765116402081,
    5.23931930405753,
    5.116781307848132,
    4.999930402879812,
    4.894979446761462,
    4.79686003515915,
    4.70983551947438,
    4.623576776987436,
    4.5463916664220845
  ],
  "val_loss_batches": [],
  "val_loss_steps": [],
  "batch_loss": [
    20.497126579284668,
    18.406490325927734,
    16.79607582092285,
    15.482303619384766,
    14.361716270446777,
    13.393843650817871,
    12.551642894744873,
    11.807844638824463,
    11.165555953979492,
    10.618160724639893,
    10.152761459350586,
    9.757619380950928,
    9.405941009521484,
    9.09318733215332,
    8.811490535736084,
    8.561145782470703,
    8.332212924957275,
    8.121989250183105,
    7.919856548309326,
    7.735702276229858,
    7.561567783355713,
    7.3955302238464355,
    7.2381932735443115,
    7.084268093109131,
    6.946149826049805,
    6.805155515670776,
    6.677734136581421,
    6.553185701370239,
    6.436058044433594,
    6.332939386367798
  ],
  "batch_step": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29
  ],
  "epoch_edges": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30
  ],
  "samples_seen": 705600,
  "lr_values": [
    8.999999421943739e-05,
    8.999997687775117e-05,
    8.99999479749463e-05,
    8.999990751103103e-05,
    8.999985548601692e-05,
    8.999979189991883e-05,
    8.999971675275488e-05,
    8.999963004454654e-05,
    8.999953177531857e-05,
    8.999942194509901e-05,
    8.999930055391921e-05,
    8.999916760181383e-05,
    8.999902308882082e-05,
    8.999886701498143e-05,
    8.999869938034021e-05,
    8.999852018494503e-05,
    8.999832942884702e-05,
    8.999812711210066e-05,
    8.999791323476368e-05,
    8.999768779689713e-05,
    8.999745079856537e-05,
    8.999720223983607e-05,
    8.999694212078018e-05,
    8.999667044147194e-05,
    8.99963872019889e-05,
    8.999609240241192e-05,
    8.999578604282517e-05,
    8.999546812331608e-05,
    8.99951386439754e-05,
    8.999479760489721e-05,
    8.999444500617886e-05,
    8.999408084792097e-05,
    8.999370513022753e-05,
    8.999331785320576e-05,
    8.999291901696623e-05,
    8.999250862162281e-05,
    8.999208666729261e-05,
    8.99916531540961e-05,
    8.999120808215704e-05,
    8.999075145160248e-05,
    8.999028326256275e-05,
    8.998980351517152e-05,
    8.998931220956573e-05,
    8.998880934588563e-05,
    8.998829492427476e-05,
    8.998776894487998e-05,
    8.998723140785142e-05,
    8.998668231334253e-05,
    8.998612166151006e-05,
    8.998554945251407e-05,
    8.998496568651788e-05,
    8.998437036368812e-05,
    8.998376348419477e-05,
    8.998314504821103e-05,
    8.998251505591345e-05,
    8.998187350748189e-05,
    8.998122040309946e-05,
    8.998055574295261e-05,
    8.997987952723106e-05,
    8.997919175612786e-05
  ],
  "lr_steps": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49,
    50,
    51,
    52,
    53,
    54,
    55,
    56,
    57,
    58,
    59
  ],
  "mb_train": {},
  "mb_val": {}
}