{
  "model_info": "DistributedDataParallel(\n  (module): YearProcessor(\n    (inner): CustomTransformer(\n      (pre_conv): Sequential(\n        (0): Conv1d(70, 1024, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(1024, 384, kernel_size=(1,), stride=(1,))\n        (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (pos_enc): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): TransformerEncoder(\n        (layers): ModuleList(\n          (0-3): 4 x TransformerEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n            )\n            (linear1): Linear(in_features=128, out_features=512, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear2): Linear(in_features=512, out_features=128, bias=True)\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (post_conv): Sequential(\n        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(256, 15, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n)",
  "train_loss": [
    11712.306884765625,
    4344.444580078125,
    4339.216064453125,
    1953.0968627929688,
    1418.9799194335938,
    1687.5120239257812,
    1376.9915161132812,
    877.5277099609375,
    647.4522705078125,
    605.6958923339844,
    575.4681396484375,
    511.58628845214844,
    399.6654052734375,
    292.0527038574219,
    249.63036346435547,
    251.31431579589844,
    240.98995208740234,
    195.57958221435547,
    155.1206817626953,
    146.17422485351562,
    142.51597595214844,
    127.02206420898438,
    111.95887756347656,
    105.00305557250977,
    97.65507507324219,
    90.0444450378418,
    85.40913391113281,
    83.2295913696289,
    76.74239349365234,
    72.01348114013672
  ],
  "val_loss": [
    2210.277500074737,
    6009.157362085459,
    2755.982643345424,
    890.9878099714007,
    1356.0234157640107,
    1526.205625884387,
    1087.0621462452168,
    846.9038752808863,
    885.7067238943918,
    904.8147566192005,
    769.2786112726951,
    565.7414531162807,
    411.73898556767676,
    359.4773241471271,
    362.04231069136637,
    344.556695105105,
    296.16914095197404,
    251.20571303854183,
    227.05231922694614,
    214.92807890444385,
    209.44929603265257,
    208.33643045230787,
    207.9332580799959,
    199.6256311202536,
    185.72059458518515,
    172.9304530124275,
    168.39329940834824,
    166.292468362925,
    165.84396255648866,
    167.30183426214725
  ],
  "val_loss_batches": [],
  "val_loss_steps": [],
  "batch_loss": [
    11712.306884765625,
    4344.444580078125,
    4339.216064453125,
    1953.0968627929688,
    1418.9799194335938,
    1687.5120239257812,
    1376.9915161132812,
    877.5277099609375,
    647.4522705078125,
    605.6958923339844,
    575.4681396484375,
    511.58628845214844,
    399.6654052734375,
    292.0527038574219,
    249.63036346435547,
    251.31431579589844,
    240.98995208740234,
    195.57958221435547,
    155.1206817626953,
    146.17422485351562,
    142.51597595214844,
    127.02206420898438,
    111.95887756347656,
    105.00305557250977,
    97.65507507324219,
    90.0444450378418,
    85.40913391113281,
    83.2295913696289,
    76.74239349365234,
    72.01348114013672
  ],
  "batch_step": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29
  ],
  "epoch_edges": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30
  ],
  "samples_seen": 705600,
  "lr_values": [
    8.999999421943739e-05,
    8.999997687775117e-05,
    8.99999479749463e-05,
    8.999990751103103e-05,
    8.999985548601692e-05,
    8.999979189991883e-05,
    8.999971675275488e-05,
    8.999963004454654e-05,
    8.999953177531857e-05,
    8.999942194509901e-05,
    8.999930055391921e-05,
    8.999916760181383e-05,
    8.999902308882082e-05,
    8.999886701498143e-05,
    8.999869938034021e-05,
    8.999852018494503e-05,
    8.999832942884702e-05,
    8.999812711210066e-05,
    8.999791323476368e-05,
    8.999768779689713e-05,
    8.999745079856537e-05,
    8.999720223983607e-05,
    8.999694212078018e-05,
    8.999667044147194e-05,
    8.99963872019889e-05,
    8.999609240241192e-05,
    8.999578604282517e-05,
    8.999546812331608e-05,
    8.99951386439754e-05,
    8.999479760489721e-05,
    8.999444500617886e-05,
    8.999408084792097e-05,
    8.999370513022753e-05,
    8.999331785320576e-05,
    8.999291901696623e-05,
    8.999250862162281e-05,
    8.999208666729261e-05,
    8.99916531540961e-05,
    8.999120808215704e-05,
    8.999075145160248e-05,
    8.999028326256275e-05,
    8.998980351517152e-05,
    8.998931220956573e-05,
    8.998880934588563e-05,
    8.998829492427476e-05,
    8.998776894487998e-05,
    8.998723140785142e-05,
    8.998668231334253e-05,
    8.998612166151006e-05,
    8.998554945251407e-05,
    8.998496568651788e-05,
    8.998437036368812e-05,
    8.998376348419477e-05,
    8.998314504821103e-05,
    8.998251505591345e-05,
    8.998187350748189e-05,
    8.998122040309946e-05,
    8.998055574295261e-05,
    8.997987952723106e-05,
    8.997919175612786e-05
  ],
  "lr_steps": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49,
    50,
    51,
    52,
    53,
    54,
    55,
    56,
    57,
    58,
    59
  ],
  "mb_train": {},
  "mb_val": {}
}