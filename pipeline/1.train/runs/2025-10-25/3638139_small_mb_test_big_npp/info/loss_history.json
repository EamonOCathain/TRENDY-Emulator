{
  "model_info": "DistributedDataParallel(\n  (module): YearProcessor(\n    (inner): CustomTransformer(\n      (pre_conv): Sequential(\n        (0): Conv1d(70, 1024, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(1024, 384, kernel_size=(1,), stride=(1,))\n        (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (pos_enc): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): TransformerEncoder(\n        (layers): ModuleList(\n          (0-3): 4 x TransformerEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n            )\n            (linear1): Linear(in_features=128, out_features=512, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear2): Linear(in_features=512, out_features=128, bias=True)\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (post_conv): Sequential(\n        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(256, 15, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n)",
  "train_loss": [
    4.639746785163879,
    3.5247464179992676,
    3.3042914867401123,
    2.4681406021118164,
    2.973354458808899,
    1.8956043720245361,
    2.1838713884353638,
    2.107858896255493,
    1.7789211869239807,
    1.8315252661705017,
    1.7112614512443542
  ],
  "val_loss": [
    1.2619749276727743,
    2.5398695482587326,
    1.574555365741253,
    2.587156682552732,
    1.8047712330003174,
    1.6857753521325638,
    1.7698036413677798,
    1.735857113215084,
    1.9839121532531416,
    1.9819894725722926,
    1.843333340915186
  ],
  "val_loss_batches": [],
  "val_loss_steps": [],
  "batch_loss": [
    4.639746785163879,
    3.5247464179992676,
    3.3042914867401123,
    2.4681406021118164,
    2.973354458808899,
    1.8956043720245361,
    2.1838713884353638,
    2.107858896255493,
    1.7789211869239807,
    1.8315252661705017,
    1.7112614512443542
  ],
  "batch_step": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10
  ],
  "epoch_edges": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11
  ],
  "samples_seen": 258720,
  "lr_values": [
    8.999999421943739e-05,
    8.999997687775117e-05,
    8.99999479749463e-05,
    8.999990751103103e-05,
    8.999985548601692e-05,
    8.999979189991883e-05,
    8.999971675275488e-05,
    8.999963004454654e-05,
    8.999953177531857e-05,
    8.999942194509901e-05,
    8.999930055391921e-05,
    8.999916760181383e-05,
    8.999902308882082e-05,
    8.999886701498143e-05,
    8.999869938034021e-05,
    8.999852018494503e-05,
    8.999832942884702e-05,
    8.999812711210066e-05,
    8.999791323476368e-05,
    8.999768779689713e-05,
    8.999745079856537e-05,
    8.999720223983607e-05
  ],
  "lr_steps": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21
  ],
  "mb_train": {
    "nbp_balance": [
      6.788826982466315e-14,
      6.235064581880476e-14,
      6.032966755830337e-14,
      5.890884390466716e-14,
      6.087544688511957e-14,
      6.49292237509013e-14,
      6.896456282700367e-14,
      7.126023387229964e-14,
      7.233935384710162e-14,
      7.212506933452286e-14,
      7.15024302380728e-14
    ],
    "carbon_partition_december": [
      2.4014901220798492,
      1.5926901698112488,
      1.4849539399147034,
      0.8227006494998932,
      1.2768344283103943,
      0.3153998851776123,
      0.5996254682540894,
      0.5462565720081329,
      0.2782013267278671,
      0.36681753396987915,
      0.29961758852005005
    ],
    "nbp_vs_delta_ctotal_monthly": [
      1.0400592684745789,
      0.8208911418914795,
      0.7249886393547058,
      0.555155873298645,
      0.5860755145549774,
      0.43577124178409576,
      0.46156883239746094,
      0.4406336843967438,
      0.3809646666049957,
      0.37511952221393585,
      0.34327076375484467
    ],
    "npp_balance": [
      1.4020094874723525e-05,
      1.381734385929883e-05,
      1.3031037601233285e-05,
      1.1888458837227468e-05,
      1.1754558329335596e-05,
      1.1869285049122431e-05,
      1.2594244232539803e-05,
      1.2546196024942985e-05,
      1.2092565007140075e-05,
      1.1990905802327015e-05,
      1.1852289072473884e-05
    ]
  },
  "mb_val": {
    "nbp_balance": [
      1.0552681013928316e-13,
      9.944006021474951e-14,
      9.736454808659453e-14,
      9.598686350171405e-14,
      9.385755642778685e-14,
      9.245159780057198e-14,
      9.25468140082134e-14,
      9.565020733937341e-14,
      1.001206992205278e-13,
      1.0386343098649988e-13,
      1.0531059170794382e-13
    ],
    "carbon_partition_december": [
      0.3629527244004493,
      1.4038484358321104,
      0.382677012653552,
      1.138573749939532,
      0.40009347311383026,
      0.28948159476465996,
      0.3405945805588402,
      0.2398955092561836,
      0.38841649652466753,
      0.354702608287685,
      0.22965302489276068
    ],
    "nbp_vs_delta_ctotal_monthly": [
      0.3027671496070237,
      0.38430794961512926,
      0.3011878613482362,
      0.43376028623992613,
      0.3019720739537698,
      0.23395306447625389,
      0.21591298272167997,
      0.22346260198867138,
      0.26462962658819267,
      0.25327810247343185,
      0.21931096994536645
    ],
    "npp_balance": [
      1.541535573549431e-05,
      1.5630166708685974e-05,
      1.4623096336363913e-05,
      1.4118880905818297e-05,
      1.3527774507621033e-05,
      1.3522147314269082e-05,
      1.3303515789248122e-05,
      1.2865817326397957e-05,
      1.2815160454984401e-05,
      1.3183030802082665e-05,
      1.3836939809677523e-05
    ]
  }
}