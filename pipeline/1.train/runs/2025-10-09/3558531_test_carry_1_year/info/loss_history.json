{
  "model_info": "DistributedDataParallel(\n  (module): YearProcessor(\n    (inner): CustomTransformer(\n      (pre_conv): Sequential(\n        (0): Conv1d(70, 1024, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(1024, 384, kernel_size=(1,), stride=(1,))\n        (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (pos_enc): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): TransformerEncoder(\n        (layers): ModuleList(\n          (0-3): 4 x TransformerEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n            )\n            (linear1): Linear(in_features=128, out_features=512, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear2): Linear(in_features=512, out_features=128, bias=True)\n            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (post_conv): Sequential(\n        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n        (1): PReLU(num_parameters=1)\n        (2): Conv1d(256, 15, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n)",
  "train_loss": [
    4.050989433236065
  ],
  "val_loss": [
    26.86504610857489
  ],
  "val_loss_batches": [],
  "val_loss_steps": [],
  "batch_loss": [
    2.6145324125176383,
    4.050989433236065,
    4.050989433236065
  ],
  "batch_step": [
    0,
    1,
    0
  ],
  "epoch_edges": [
    0,
    3
  ],
  "samples_seen": 5880,
  "lr_values": [
    8.999999421943739e-05,
    8.999997687775117e-05,
    8.99999479749463e-05,
    8.999990751103103e-05,
    8.999985548601692e-05,
    8.999979189991883e-05,
    8.999971675275488e-05,
    8.999963004454654e-05,
    8.999953177531857e-05,
    8.999942194509901e-05,
    8.999930055391921e-05,
    8.999916760181383e-05,
    8.999902308882082e-05,
    8.999886701498143e-05,
    8.999869938034021e-05,
    8.999852018494503e-05,
    8.999832942884702e-05,
    8.999812711210066e-05,
    8.999791323476368e-05,
    8.999768779689713e-05,
    8.999745079856537e-05,
    8.999720223983607e-05,
    8.999694212078018e-05,
    8.999667044147194e-05,
    8.99963872019889e-05,
    8.999609240241192e-05,
    8.999578604282517e-05,
    8.999546812331608e-05,
    8.99951386439754e-05,
    8.999479760489721e-05,
    8.999444500617886e-05,
    8.999408084792097e-05,
    8.999370513022753e-05,
    8.999331785320576e-05,
    8.999291901696623e-05,
    8.999250862162281e-05,
    8.999208666729261e-05,
    8.99916531540961e-05,
    8.999120808215704e-05,
    8.999075145160248e-05,
    8.999028326256275e-05,
    8.998980351517152e-05,
    8.998931220956573e-05,
    8.998880934588563e-05,
    8.998829492427476e-05,
    8.998776894487998e-05,
    8.998723140785142e-05,
    8.998668231334253e-05,
    8.998612166151006e-05,
    8.998554945251407e-05,
    8.998496568651788e-05,
    8.998437036368812e-05,
    8.998376348419477e-05,
    8.998314504821103e-05,
    8.998251505591345e-05,
    8.998187350748189e-05,
    8.998122040309946e-05,
    8.998055574295261e-05,
    8.997987952723106e-05,
    8.997919175612786e-05,
    8.997849242983933e-05,
    8.99777815485651e-05,
    8.997705911250811e-05,
    8.997632512187455e-05,
    8.9975579576874e-05,
    8.997482247771923e-05,
    8.997405382462641e-05,
    8.997327361781492e-05,
    8.997248185750749e-05,
    8.997167854393015e-05,
    8.997086367731218e-05,
    8.997003725788625e-05,
    8.996919928588821e-05,
    8.99683497615573e-05,
    8.996748868513602e-05,
    8.996661605687015e-05,
    8.996573187700882e-05,
    8.996483614580442e-05,
    8.996392886351264e-05,
    8.996301003039248e-05,
    8.996207964670623e-05,
    8.996113771271946e-05,
    8.996018422870109e-05,
    8.995921919492325e-05,
    8.995824261166145e-05,
    8.995725447919449e-05,
    8.995625479780437e-05,
    8.995524356777653e-05,
    8.995422078939962e-05,
    8.995318646296558e-05,
    8.995214058876968e-05,
    8.995108316711047e-05,
    8.99500141982898e-05,
    8.994893368261286e-05,
    8.994784162038802e-05,
    8.994673801192708e-05,
    8.994562285754503e-05,
    8.994449615756025e-05,
    8.994335791229434e-05,
    8.994220812207222e-05,
    8.994104678722211e-05,
    8.993987390807554e-05,
    8.993868948496731e-05,
    8.993749351823552e-05,
    8.993628600822158e-05,
    8.993506695527017e-05,
    8.993383635972931e-05,
    8.993259422195027e-05,
    8.993134054228761e-05,
    8.993007532109923e-05,
    8.99287985587463e-05,
    8.992751025559328e-05,
    8.992621041200792e-05,
    8.992489902836128e-05,
    8.992357610502772e-05,
    8.992224164238485e-05,
    8.992089564081364e-05,
    8.99195381006983e-05,
    8.991816902242636e-05,
    8.991678840638863e-05,
    8.991539625297922e-05,
    8.991399256259555e-05,
    8.99125773356383e-05,
    8.991115057251146e-05,
    8.990971227362234e-05,
    8.990826243938147e-05,
    8.990680107020277e-05,
    8.990532816650337e-05,
    8.990384372870373e-05,
    8.99023477572276e-05,
    8.990084025250203e-05,
    8.989932121495734e-05,
    8.989779064502716e-05,
    8.989624854314839e-05,
    8.989469490976128e-05,
    8.989312974530927e-05,
    8.989155305023919e-05,
    8.988996482500112e-05,
    8.988836507004844e-05,
    8.98867537858378e-05,
    8.988513097282918e-05,
    8.988349663148578e-05,
    8.988185076227419e-05,
    8.988019336566422e-05,
    8.987852444212899e-05,
    8.987684399214492e-05,
    8.98751520161917e-05,
    8.987344851475231e-05,
    8.987173348831305e-05,
    8.987000693736349e-05,
    8.98682688623965e-05,
    8.98665192639082e-05,
    8.986475814239806e-05,
    8.986298549836878e-05,
    8.98612013323264e-05,
    8.985940564478024e-05,
    8.985759843624288e-05,
    8.98557797072302e-05,
    8.985394945826138e-05,
    8.985210768985888e-05,
    8.985025440254847e-05,
    8.984838959685915e-05,
    8.98465132733233e-05,
    8.984462543247648e-05,
    8.984272607485764e-05,
    8.984081520100894e-05,
    8.983889281147588e-05,
    8.983695890680719e-05
  ],
  "lr_steps": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49,
    50,
    51,
    52,
    53,
    54,
    55,
    56,
    57,
    58,
    59,
    60,
    61,
    62,
    63,
    64,
    65,
    66,
    67,
    68,
    69,
    70,
    71,
    72,
    73,
    74,
    75,
    76,
    77,
    78,
    79,
    80,
    81,
    82,
    83,
    84,
    85,
    86,
    87,
    88,
    89,
    90,
    91,
    92,
    93,
    94,
    95,
    96,
    97,
    98,
    99,
    100,
    101,
    102,
    103,
    104,
    105,
    106,
    107,
    108,
    109,
    110,
    111,
    112,
    113,
    114,
    115,
    116,
    117,
    118,
    119,
    120,
    121,
    122,
    123,
    124,
    125,
    126,
    127,
    128,
    129,
    130,
    131,
    132,
    133,
    134,
    135,
    136,
    137,
    138,
    139,
    140,
    141,
    142,
    143,
    144,
    145,
    146,
    147,
    148,
    149,
    150,
    151,
    152,
    153,
    154,
    155,
    156,
    157,
    158,
    159,
    160,
    161,
    162,
    163,
    164,
    165,
    166,
    167
  ],
  "carry_stage_steps": [
    0
  ],
  "carry_stage_epochs": [
    0
  ],
  "carry_stage_values": [
    1.0
  ]
}